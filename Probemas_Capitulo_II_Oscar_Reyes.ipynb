{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Mahtematics for Machine Learning**\n",
        "\n",
        "#**Book problems solved**\n",
        "\n",
        "##**Problems - Chapter two**\n",
        "\n",
        "##**Author: Oscar Reyes**\n",
        "\n",
        "##**2024 - II**\n",
        "\n",
        "\n",
        "____________________"
      ],
      "metadata": {
        "id": "voshqp7NMRrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 2.1**\n",
        "\n",
        "To walk “downhill” on the loss function (equation 2.5), we measure its gradient with respect to the parameters $\\phi_0$ and $\\phi_1$. Calculate expressions for the slopes $\\partial L / \\partial \\phi_0$ and $\\partial L / \\partial \\phi_1$.\n",
        "\n",
        "Answer:\n",
        "\n",
        "#$\\frac{\\partial L}{\\partial \\phi_0} = 2(\\phi_0 + \\phi_1x_i - y_i)$\n",
        "\n",
        "#$\\frac{\\partial L}{\\partial \\phi_1} = 2x_i(\\phi_0 + \\phi_1x_i - y_i)$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_IA53cdPVIuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 2.2**\n",
        "\n",
        "Show that we can find the minimum of the loss function in closed form by setting the expression for the derivatives from problem 2.1 to zero and solving for $\\phi_0$ and $\\phi_1$. Note that this works for linear regression but not for more complex models; this is why we use iterative model fitting methods like gradient descent (figure 2.4).\n",
        "\n",
        "Answer:\n",
        "\n",
        "#For $\\phi_0$:\n",
        "\n",
        "#$\\frac{\\partial L}{\\partial \\phi_0} = 2(\\phi_0 + \\phi_1x_i - y_i) = 0$\n",
        "\n",
        "#$(\\phi_0 + \\phi_1x_i - y_i) = 0$\n",
        "\n",
        "#$\\phi_0 = y_i - \\phi_1x_i$\n",
        "\n",
        "\n",
        "#For $\\phi_1$:\n",
        "\n",
        "#$\\frac{\\partial L}{\\partial \\phi_1} = 2x_i(\\phi_0 + \\phi_1x_i - y_i) = 0$\n",
        "\n",
        "#$(\\phi_0 + \\phi_1x_i - y_i) = 0$\n",
        "\n",
        "#$\\phi_1 = \\frac{y_i - \\phi_0}{x_i}$\n",
        "\n",
        "#Note: it works for linear regression but not for more complex models; this is why we use iterative model fitting methods like gradient descent (figure 2.4)."
      ],
      "metadata": {
        "id": "H-_WPKrccIUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 2.3**\n",
        "\n",
        "Consider reformulating linear regression as a generative model, so we have $x=g[y, \\phi] = \\phi_0 + \\phi_1y$. What is the new loss function? Find an expression for the inverse function $y = g^{-1}[x, \\phi]$ that we would use to perform inference. Will this model make the same predictions as the discriminative version for a given training dataset {$x_i, y_i$}? One way to establish this is to write code that fits a line to three data points using both methods and see if the result is the same.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Given $x=g[y, \\phi] = \\phi_0 + \\phi_1y$ ,\n",
        "\n",
        "The inverse function will be:\n",
        "\n",
        "$x-\\phi_0=\\phi_1y$\n",
        "\n",
        "$y=\\frac{x - \\phi_0}{\\phi_1}$\n",
        "\n",
        "So this expression represents the inverse function\n",
        "\n",
        "          "
      ],
      "metadata": {
        "id": "VCQKr4WgiddA"
      }
    }
  ]
}